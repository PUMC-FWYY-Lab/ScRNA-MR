{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5bdf6a5-5e08-4f37-aff9-e221ef100abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-16 21:31:43,184\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2023-11-16 21:31:43,598\tINFO util.py:159 -- Outdated packages:\n",
      "  ipywidgets==7.7.0 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import pyarrow # must occur prior to ray import\n",
    "import ray\n",
    "from ray import tune\n",
    "from ray.tune import ExperimentAnalysis\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns; sns.set()\n",
    "from collections import Counter\n",
    "from datasets import load_from_disk\n",
    "from scipy.stats import ranksums\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import Trainer\n",
    "from transformers.training_args import TrainingArguments\n",
    "\n",
    "from geneformer import DataCollatorForCellClassification\n",
    "from geneformer import TranscriptomeTokenizer\n",
    "import anndata as ad\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7958d640-6b79-4614-9787-3368e6a5c650",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset=load_from_disk(\"/root/autodl-tmp/B_cell.dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4db9c6c-831a-47c6-b0e1-f6e8c2de3c87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'cell_type', 'condition', 'length'],\n",
       "    num_rows: 381\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_proc=30\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c1cdecc-bc9e-4db2-8ece-eed173fff387",
   "metadata": {},
   "outputs": [],
   "source": [
    "conditions = train_dataset[\"condition\"]\n",
    "target_names = list(set(conditions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26cf5428-85ac-4f89-b5b7-009e7f55355d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ctrl', 'AF']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5f03dc8-38ff-4f3f-93f8-f42d668b3493",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_name_id_dict = dict(zip(target_names,[i for i in range(len(target_names))]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5747858-9ad4-4c07-8a2e-fc0509b6c10d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Ctrl': 0, 'AF': 1}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_name_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0c73cae-a840-4422-9ae2-4bdf58ce9a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset_v3 = train_dataset.rename_column(\"condition\",\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d976d8-4716-4b2e-af02-9b07c926afc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'cell_type', 'label', 'length'],\n",
       "    num_rows: 381\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a996e2ee-da98-405f-ba56-88f920dedff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb53ff78a6b64d98a4d10fb5b0578e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=30):   0%|          | 0/381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def classes_to_ids(example):\n",
    "    example[\"label\"] = target_name_id_dict[example[\"label\"]]\n",
    "    return example\n",
    "\n",
    "trainset_v4 = trainset_v3.map(classes_to_ids, num_proc=num_proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb3d6b09-bc9c-4dd8-b158-20337dfcafe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "indiv_list = trainset_v4[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "afde83b0-61b6-4659-8f6b-80fdf6cb8384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "381"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(indiv_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3b20a793-67ed-425d-ac54-91e37e4d79d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "train_indiv = random.sample(indiv_list,round(0.7*len(indiv_list)))\n",
    "eval_indiv = [indiv for indiv in indiv_list if indiv not in train_indiv]\n",
    "valid_indiv = random.sample(eval_indiv,round(0.5*len(eval_indiv)))\n",
    "test_indiv = [indiv for indiv in eval_indiv if indiv not in valid_indiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9a015f0-d3c0-446a-85e3-d677f1294f31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dc530d750944172a200859dae45b3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=30):   0%|          | 0/381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def if_train(example):\n",
    "    return example[\"input_ids\"] in train_indiv\n",
    "\n",
    "classifier_trainset = trainset_v4.filter(if_train,num_proc=num_proc).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e7bd24d-62cc-4a28-868f-29ad36481c20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cde00e564938441ba525aac8fb052058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=30):   0%|          | 0/381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def if_valid(example):\n",
    "    return example[\"input_ids\"] in valid_indiv\n",
    "\n",
    "classifier_validset = trainset_v4.filter(if_valid,num_proc=num_proc).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bcf7ea74-8fc1-4cdb-8043-5f53591d9380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0abf265d4f64789a1d627df003dd6f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter (num_proc=30):   0%|          | 0/381 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/root/miniconda3/lib/python3.8/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by mode='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    }
   ],
   "source": [
    "def if_test(example):\n",
    "    return example[\"input_ids\"] in test_indiv\n",
    "\n",
    "classifier_testdset = trainset_v4.filter(if_test,num_proc=num_proc).shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df0e349-3af0-40fd-9b45-d5e8d8fdcd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = datetime.datetime.now()\n",
    "datestamp = f\"{str(current_date.year)[-2:]}{current_date.month:02d}{current_date.day:02d}\"\n",
    "output_dir = f\"/root/autodl-tmp/B_cell/model/{datestamp}_geneformer_DiseaseClassifier/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a444ca9e-e70e-48e1-a60f-32e50b9ca3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "saved_model_test = os.path.join(output_dir, f\"pytorch_model.bin\")\n",
    "if os.path.isfile(saved_model_test) == True:\n",
    "    raise Exception(\"Model already saved to this directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "653f7145-e0d1-4076-9e92-8969ede23732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make output directory\n",
    "import subprocess\n",
    "subprocess.call(f'mkdir {output_dir}', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31f18f11-28f9-4f89-a0be-8df786cd4e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "# how many pretrained layers to freeze\n",
    "freeze_layers = 2\n",
    "# batch size for training and eval\n",
    "geneformer_batch_size = 12\n",
    "# number of epochs\n",
    "epochs = 2\n",
    "# logging steps\n",
    "logging_steps = round(len(classifier_trainset)/geneformer_batch_size/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ae56ff5-877d-452b-9e10-bbeeaef7f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init():\n",
    "    model = BertForSequenceClassification.from_pretrained(\"/root/autodl-tmp/Geneformer/\",\n",
    "                                                          num_labels=len(target_names),\n",
    "                                                          output_attentions = False,\n",
    "                                                          output_hidden_states = False)\n",
    "    if freeze_layers is not None:\n",
    "        modules_to_freeze = model.bert.encoder.layer[:freeze_layers]\n",
    "        for module in modules_to_freeze:\n",
    "            for param in module.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20478f67-e642-4811-bf13-60a1af9ef177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy and macro f1 using sklearn's function\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    macro_f1 = f1_score(labels, preds, average='macro')\n",
    "    return {\n",
    "      'accuracy': acc,\n",
    "      'macro_f1': macro_f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cee6ed77-9380-4a5a-a2d5-fcdecbf5020d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = {\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"evaluation_strategy\": \"steps\",\n",
    "    \"eval_steps\": logging_steps,\n",
    "    \"logging_steps\": logging_steps,\n",
    "    \"group_by_length\": True,\n",
    "    \"length_column_name\": \"length\",\n",
    "    \"disable_tqdm\": True,\n",
    "    \"skip_memory_metrics\": True, # memory tracker causes errors in raytune\n",
    "    \"per_device_train_batch_size\": geneformer_batch_size,\n",
    "    \"per_device_eval_batch_size\": geneformer_batch_size,\n",
    "    \"num_train_epochs\": epochs,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"output_dir\": output_dir,\n",
    "    \"save_steps\":2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "16de85cd-43eb-45ff-a24a-50446faff3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args_init = TrainingArguments(**training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "906891ce-cdbf-4284-9e02-b6717558c03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/autodl-tmp/Geneformer/ and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model_init=model_init,\n",
    "    args=training_args_init,\n",
    "    data_collator=DataCollatorForCellClassification(),\n",
    "    train_dataset=classifier_trainset,\n",
    "    eval_dataset=classifier_validset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f458b52-0315-4cef-9617-35055ee297e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_config = {\n",
    "    \"num_train_epochs\": tune.choice([epochs]),\n",
    "    \"learning_rate\": tune.loguniform(1e-6, 1e-3),\n",
    "    \"weight_decay\": tune.uniform(0.0, 0.3),\n",
    "    \"lr_scheduler_type\": tune.choice([\"linear\",\"cosine\",\"polynomial\"]),\n",
    "    \"warmup_steps\": tune.uniform(100, 2000),\n",
    "    \"seed\": tune.uniform(0,100),\n",
    "    \"per_device_train_batch_size\": tune.choice([geneformer_batch_size])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a53f4eb9-36f8-45b8-9f87-b5f64b9b6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperopt_search = HyperOptSearch(\n",
    "    metric=\"eval_accuracy\", mode=\"max\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f04fe815-6024-44af-8b5c-457801af6f63",
   "metadata": {},
   "outputs": [
    {
     "ename": "DeprecationWarning",
     "evalue": "Accepting a `checkpoint_dir` argument in your training function is deprecated.\nPlease use `ray.train.get_checkpoint()` to access your checkpoint as a\n`ray.train.Checkpoint` object instead. See below for an example:\n\nBefore\n------\n\nfrom ray import tune\n\ndef train_fn(config, checkpoint_dir=None):\n    if checkpoint_dir:\n        torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n    ...\n\ntuner = tune.Tuner(train_fn)\ntuner.fit()\n\nAfter\n-----\n\nfrom ray import train, tune\n\ndef train_fn(config):\n    checkpoint: train.Checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n    ...\n\ntuner = tune.Tuner(train_fn)\ntuner.fit()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mDeprecationWarning\u001b[0m                        Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# optimize hyperparameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhyperparameter_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbackend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mray\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresources_per_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpu\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhp_space\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mray_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msearch_alg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhyperopt_search\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# number of trials\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprogress_reporter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLIReporter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_report_frequency\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43msort_by_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmax_progress_rows\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                                   \u001b[49m\u001b[43mmetric_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_accuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmacro_f1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/trainer.py:2625\u001b[0m, in \u001b[0;36mTrainer.hyperparameter_search\u001b[0;34m(self, hp_space, compute_objective, n_trials, direction, backend, hp_name, **kwargs)\u001b[0m\n\u001b[1;32m   2622\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_name \u001b[38;5;241m=\u001b[39m hp_name\n\u001b[1;32m   2623\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_objective \u001b[38;5;241m=\u001b[39m default_compute_objective \u001b[38;5;28;01mif\u001b[39;00m compute_objective \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m compute_objective\n\u001b[0;32m-> 2625\u001b[0m best_run \u001b[38;5;241m=\u001b[39m \u001b[43mbackend_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhp_search_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2628\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_run\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/hyperparameter_search.py:87\u001b[0m, in \u001b[0;36mRayTuneBackend.run\u001b[0;34m(self, trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, trainer, n_trials: \u001b[38;5;28mint\u001b[39m, direction: \u001b[38;5;28mstr\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_hp_search_ray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/integrations/integration_utils.py:330\u001b[0m, in \u001b[0;36mrun_hp_search_ray\u001b[0;34m(trainer, n_trials, direction, **kwargs)\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    319\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m], (ASHAScheduler, MedianStoppingRule, HyperBandForBOHB, PopulationBasedTraining)\n\u001b[1;32m    320\u001b[0m     ) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdo_eval \u001b[38;5;129;01mor\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mevaluation_strategy \u001b[38;5;241m==\u001b[39m IntervalStrategy\u001b[38;5;241m.\u001b[39mNO):\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    322\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are using \u001b[39m\u001b[38;5;132;01m{cls}\u001b[39;00m\u001b[38;5;124m as a scheduler but you haven\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt enabled evaluation during training. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    323\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis means your trials will not report intermediate results to Ray Tune, and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrainer `args`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtype\u001b[39m(kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscheduler\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    328\u001b[0m         )\n\u001b[0;32m--> 330\u001b[0m trainable \u001b[38;5;241m=\u001b[39m \u001b[43mray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_objective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_trainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(trainable)\n\u001b[1;32m    333\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdynamic_modules_import_trainable\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[38;5;124;03m    Wrapper around `tune.with_parameters` to ensure datasets_modules are loaded on each Actor.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;124;03m    Assumes that `_objective`, defined above, is a function.\u001b[39;00m\n\u001b[1;32m    340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.8/site-packages/ray/tune/trainable/util.py:315\u001b[0m, in \u001b[0;36mwith_parameters\u001b[0;34m(trainable, **kwargs)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _detect_checkpoint_function(trainable, partial\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtune\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunction_trainable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    312\u001b[0m         _CHECKPOINT_DIR_ARG_DEPRECATION_MSG,\n\u001b[1;32m    313\u001b[0m     )\n\u001b[0;32m--> 315\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(_CHECKPOINT_DIR_ARG_DEPRECATION_MSG)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(config):\n\u001b[1;32m    318\u001b[0m     fn_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mDeprecationWarning\u001b[0m: Accepting a `checkpoint_dir` argument in your training function is deprecated.\nPlease use `ray.train.get_checkpoint()` to access your checkpoint as a\n`ray.train.Checkpoint` object instead. See below for an example:\n\nBefore\n------\n\nfrom ray import tune\n\ndef train_fn(config, checkpoint_dir=None):\n    if checkpoint_dir:\n        torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n    ...\n\ntuner = tune.Tuner(train_fn)\ntuner.fit()\n\nAfter\n-----\n\nfrom ray import train, tune\n\ndef train_fn(config):\n    checkpoint: train.Checkpoint = train.get_checkpoint()\n    if checkpoint:\n        with checkpoint.as_directory() as checkpoint_dir:\n            torch.load(os.path.join(checkpoint_dir, \"checkpoint.pt\"))\n    ...\n\ntuner = tune.Tuner(train_fn)\ntuner.fit()"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters\n",
    "trainer.hyperparameter_search(\n",
    "    direction=\"maximize\",\n",
    "    backend=\"ray\",\n",
    "    resources_per_trial={\"cpu\":8,\"gpu\":1},\n",
    "    hp_space=lambda _: ray_config,\n",
    "    search_alg=hyperopt_search,\n",
    "    n_trials=100, # number of trials\n",
    "    progress_reporter=tune.CLIReporter(max_report_frequency=600,\n",
    "                                                   sort_by_metric=True,\n",
    "                                                   max_progress_rows=100,\n",
    "                                                   mode=\"max\",\n",
    "                                                   metric=\"eval_accuracy\",\n",
    "                                                   metric_columns=[\"loss\", \"eval_loss\", \"eval_accuracy\",\"macro_f1\"])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a6bd5e44-14df-447e-a31c-0ba6dd10b20f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at /root/autodl-tmp/Geneformer/ and are newly initialized: ['classifier.weight', 'bert.pooler.dense.weight', 'bert.pooler.dense.bias', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6848, 'learning_rate': 4.782608695652174e-05, 'epoch': 0.09}\n",
      "{'eval_loss': 0.6521574854850769, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3812, 'eval_samples_per_second': 149.514, 'eval_steps_per_second': 13.115, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7019, 'learning_rate': 4.565217391304348e-05, 'epoch': 0.17}\n",
      "{'eval_loss': 0.6317639350891113, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3857, 'eval_samples_per_second': 147.795, 'eval_steps_per_second': 12.964, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6518, 'learning_rate': 4.347826086956522e-05, 'epoch': 0.26}\n",
      "{'eval_loss': 0.6273093819618225, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3825, 'eval_samples_per_second': 149.009, 'eval_steps_per_second': 13.071, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6362, 'learning_rate': 4.130434782608696e-05, 'epoch': 0.35}\n",
      "{'eval_loss': 0.6169483661651611, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.382, 'eval_samples_per_second': 149.227, 'eval_steps_per_second': 13.09, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.612, 'learning_rate': 3.91304347826087e-05, 'epoch': 0.43}\n",
      "{'eval_loss': 0.6050921082496643, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3845, 'eval_samples_per_second': 148.232, 'eval_steps_per_second': 13.003, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6489, 'learning_rate': 3.695652173913043e-05, 'epoch': 0.52}\n",
      "{'eval_loss': 0.5949985384941101, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3823, 'eval_samples_per_second': 149.094, 'eval_steps_per_second': 13.078, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6118, 'learning_rate': 3.478260869565218e-05, 'epoch': 0.61}\n",
      "{'eval_loss': 0.5869174599647522, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3835, 'eval_samples_per_second': 148.646, 'eval_steps_per_second': 13.039, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5468, 'learning_rate': 3.260869565217392e-05, 'epoch': 0.7}\n",
      "{'eval_loss': 0.5790730714797974, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3832, 'eval_samples_per_second': 148.754, 'eval_steps_per_second': 13.049, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5256, 'learning_rate': 3.0434782608695656e-05, 'epoch': 0.78}\n",
      "{'eval_loss': 0.5716580748558044, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3837, 'eval_samples_per_second': 148.545, 'eval_steps_per_second': 13.03, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7009, 'learning_rate': 2.826086956521739e-05, 'epoch': 0.87}\n",
      "{'eval_loss': 0.5665240287780762, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3828, 'eval_samples_per_second': 148.918, 'eval_steps_per_second': 13.063, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7637, 'learning_rate': 2.608695652173913e-05, 'epoch': 0.96}\n",
      "{'eval_loss': 0.5644032955169678, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3843, 'eval_samples_per_second': 148.325, 'eval_steps_per_second': 13.011, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6741, 'learning_rate': 2.391304347826087e-05, 'epoch': 1.04}\n",
      "{'eval_loss': 0.5631766319274902, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3828, 'eval_samples_per_second': 148.887, 'eval_steps_per_second': 13.06, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6734, 'learning_rate': 2.173913043478261e-05, 'epoch': 1.13}\n",
      "{'eval_loss': 0.5623306035995483, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3835, 'eval_samples_per_second': 148.631, 'eval_steps_per_second': 13.038, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6652, 'learning_rate': 1.956521739130435e-05, 'epoch': 1.22}\n",
      "{'eval_loss': 0.5617259740829468, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3838, 'eval_samples_per_second': 148.499, 'eval_steps_per_second': 13.026, 'epoch': 1.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4827, 'learning_rate': 1.739130434782609e-05, 'epoch': 1.3}\n",
      "{'eval_loss': 0.5607434511184692, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3823, 'eval_samples_per_second': 149.112, 'eval_steps_per_second': 13.08, 'epoch': 1.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.661, 'learning_rate': 1.5217391304347828e-05, 'epoch': 1.39}\n",
      "{'eval_loss': 0.5601091980934143, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3828, 'eval_samples_per_second': 148.892, 'eval_steps_per_second': 13.061, 'epoch': 1.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5814, 'learning_rate': 1.3043478260869566e-05, 'epoch': 1.48}\n",
      "{'eval_loss': 0.5597999095916748, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3823, 'eval_samples_per_second': 149.109, 'eval_steps_per_second': 13.08, 'epoch': 1.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6262, 'learning_rate': 1.0869565217391305e-05, 'epoch': 1.57}\n",
      "{'eval_loss': 0.5595527291297913, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3833, 'eval_samples_per_second': 148.719, 'eval_steps_per_second': 13.046, 'epoch': 1.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6908, 'learning_rate': 8.695652173913044e-06, 'epoch': 1.65}\n",
      "{'eval_loss': 0.5595692992210388, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3837, 'eval_samples_per_second': 148.543, 'eval_steps_per_second': 13.03, 'epoch': 1.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5525, 'learning_rate': 6.521739130434783e-06, 'epoch': 1.74}\n",
      "{'eval_loss': 0.5594095587730408, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3844, 'eval_samples_per_second': 148.29, 'eval_steps_per_second': 13.008, 'epoch': 1.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5326, 'learning_rate': 4.347826086956522e-06, 'epoch': 1.83}\n",
      "{'eval_loss': 0.5591844320297241, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3848, 'eval_samples_per_second': 148.131, 'eval_steps_per_second': 12.994, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5461, 'learning_rate': 2.173913043478261e-06, 'epoch': 1.91}\n",
      "{'eval_loss': 0.5589110851287842, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3841, 'eval_samples_per_second': 148.393, 'eval_steps_per_second': 13.017, 'epoch': 1.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9017, 'learning_rate': 0.0, 'epoch': 2.0}\n",
      "{'eval_loss': 0.5588369369506836, 'eval_accuracy': 0.7368421052631579, 'eval_macro_f1': 0.4242424242424242, 'eval_runtime': 0.3847, 'eval_samples_per_second': 148.174, 'eval_steps_per_second': 12.998, 'epoch': 2.0}\n",
      "{'train_runtime': 20.6609, 'train_samples_per_second': 25.846, 'train_steps_per_second': 2.226, 'train_loss': 0.637907950774483, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=46, training_loss=0.637907950774483, metrics={'train_runtime': 20.6609, 'train_samples_per_second': 25.846, 'train_steps_per_second': 2.226, 'train_loss': 0.637907950774483, 'epoch': 2.0})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c7d564f-8209-4cc1-a059-2109e99d951a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/geneformer/collator_for_classification.py:581: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  batch = {k: torch.tensor(v, dtype=torch.int64) for k, v in batch.items()}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(classifier_testdset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "fc8d79d4-ff86-4e19-a263-abfc702e0f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f\"{output_dir}predictions.pickle\", \"wb\") as fp:\n",
    "        pickle.dump(predictions, fp)\n",
    "trainer.save_metrics(\"eval\",predictions.metrics)\n",
    "trainer.save_model(output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
